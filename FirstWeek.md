Hi, Xiyi Tang:

 附件中

 1 是 Attention的一篇经典论文以及 

 2 4篇关于Attention可解释性的论文

 https://new.qq.com/rain/a/20190928A0KF3S00 是一个简单BLOG，可以用于快速阅读，最终还是要附件中论文认真看一下。

 Attention最初从CV引入NLP领域中就是利用了RNNs的结构来完成的（这一过程请具体参考NMT的这篇论文）

 

 那么我们期待通过论述RNNs引入Attention与未引入Attention这两种情况下 RNNs隐含层的区别来讨论这种可解释性。

下面的几个BLOG是关于RNNs写的比较好的BLOG

http://karpathy.github.io/2015/05/21/rnn-effectiveness/

http://colah.github.io/posts/2015-08-Understanding-LSTMs/ （这个人写的一系列BLOG你都可以看一下，用于增强对NLP的理解认知）

https://medium.com/explore-artificial-intelligence/an-introduction-to-recurrent-neural-networks-72c97bf0912

http://karpathy.github.io/2015/05/21/rnn-effectiveness/

 前期工作如下：

 1 阅读附件中论文，并写好review

 2 调研现有Attention的可视化方法

 3 从GITHUB中寻找NMT的源码，完成初步的训练过程，用于后期探究隐含层区别。

Best

Cheng.Zhang